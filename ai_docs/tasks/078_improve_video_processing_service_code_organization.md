# Python AI Task Template

> **Instructions:** This template helps you create comprehensive task documents for AI-driven Python development. Fill out each section thoroughly to ensure the AI agent has all necessary context and can execute the task systematically.

---

## 1. Task Overview

### Task Title
**Title:** Improve Video Processing Service Code Organization and Reliability

### Goal Statement
**Goal:** Refactor the video processing service to improve code maintainability, implement sequential chunk processing to prevent resource conflicts, add proper configuration management, and enhance reliability through structured error handling with retry logic. This will make the service more robust, easier to maintain, and better aligned with existing architectural patterns while preventing file locking issues and memory pressure.

---

## 2. MANDATORY: Existing Codebase Analysis

### üö® CRITICAL WORKFLOW REQUIREMENT
**‚ö†Ô∏è BEFORE ANY PLANNING OR IMPLEMENTATION: You MUST thoroughly analyze the existing codebase to understand:**

‚úÖ **ANALYSIS COMPLETED** - The following analysis has been conducted:

1. **What services/modules already exist** - ‚úÖ Found `VideoProcessingService` in `video_processing_service.py`
2. **How the current workflow processes** - ‚úÖ Analyzed existing video chunk processing pipeline
3. **Whether this is an extension** - ‚úÖ This extends existing `VideoProcessingService` 
4. **What patterns and architectures** - ‚úÖ Identified existing config patterns and service architecture

### Existing Services & Modules Analysis

#### Step 1: Project Structure Discovery - ‚úÖ COMPLETED
```
apps/rag-processor/
‚îú‚îÄ‚îÄ rag_processor/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Main configuration class
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_processing_service.py  # TARGET FOR IMPROVEMENT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processing_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedding_service.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ config_utils.py       # Contains RetryConfig and TimeoutConfig
‚îÇ       ‚îî‚îÄ‚îÄ retry_utils.py        # Has NonRetryableError base class
```

#### Step 2: Related Service Discovery - ‚úÖ COMPLETED
**FOUND RELATED SERVICES:**
- ‚úÖ `VideoProcessingService` - Primary target for improvements
- ‚úÖ `ProcessingService` - Calls video processing service 
- ‚úÖ `DatabaseService` - Stores processed chunks
- ‚úÖ `EmbeddingService` - Generates dual embeddings (text + multimodal)
- ‚úÖ `AudioTranscriptionService` - Used by video service for transcription

#### Step 3: Current Workflow Understanding - ‚úÖ COMPLETED
**CURRENT FLOW:**
```
Entry Point: VideoProcessingService.process_video_file()
Processing Steps: 
1. Get video duration ‚Üí 2. Create 30-second chunks (PARALLEL) ‚Üí 
3. For each chunk: transcribe + generate context (PARALLEL) ‚Üí 
4. Generate dual embeddings (PARALLEL) ‚Üí 5. Store in database

Database Operations: ChunkData stored with text/context/dual embeddings
Output/Response: List of processed ChunkData objects
```

#### Step 4: Integration vs New Code Decision - ‚úÖ COMPLETED
**üéØ INTEGRATION DECISION MATRIX:**

**‚úÖ EXTEND EXISTING SERVICE** - VideoProcessingService improvements chosen because:
- [x] Video processing functionality already exists and works well
- [x] The workflow improvements fit naturally into existing processing pipeline  
- [x] Adding configuration and retry logic maintains consistency with established patterns
- [x] Sequential processing is a refinement, not a complete rewrite

**üìã ANALYSIS RESULTS:**
- **Existing Related Services:** VideoProcessingService (primary), ProcessingService, DatabaseService, EmbeddingService
- **Current Workflow:** 30-second video chunks ‚Üí parallel transcription+context ‚Üí dual embeddings ‚Üí database storage  
- **Integration Decision:** Extend existing VideoProcessingService with better organization and reliability
- **Recommended Entry Point:** Refactor VideoProcessingService._process_chunks method and add configuration constants

### Existing Technology Stack - ‚úÖ ANALYZED
- **Python Version:** 3.10+ (from deployment config)
- **Primary Framework:** FastAPI with async/await patterns
- **Database:** PostgreSQL with direct psycopg2 connections + vector support
- **Existing AI/ML Services:** 
  - Google GenAI for video context generation (gemini-2.5-flash)
  - VertexAI for multimodal embeddings (multimodalembedding@001) 
  - AudioTranscriptionService for speech-to-text
- **Authentication Patterns:** Google GenAI API key + Vertex AI service account
- **Processing Pipeline:** Async parallel processing with proper cleanup and error handling

### üö® INTEGRATION REQUIREMENTS
**Based on analysis:**
- **Files to Modify:** 
  - `apps/rag-processor/rag_processor/config.py` - Add video processing constants
  - `apps/rag-processor/rag_processor/services/video_processing_service.py` - Main refactoring target
- **New Files Needed:** None - extending existing service
- **Dependencies to Add:** None - using existing packages and retry utilities
- **Migration Needed:** None - backward compatible improvements

---

## 3. Strategic Analysis & Solution Options

### Problem Context
The VideoProcessingService has excellent functionality but suffers from maintainability and reliability issues: large monolithic methods (100+ lines), parallel chunk creation that can cause file locking, hard-coded configuration values, and no structured retry logic. These issues make the service harder to debug, test, and maintain while potentially causing resource conflicts during processing.

### Solution Options Analysis

#### Option 1: Comprehensive Refactoring with Sequential Processing (Recommended)
**Approach:** Break large methods into focused functions, implement sequential chunk processing, add configuration management, and integrate existing retry patterns

**Pros:**
- ‚úÖ **Eliminates file locking issues** - Sequential chunk creation prevents ffmpeg conflicts
- ‚úÖ **Better maintainability** - Smaller, focused methods easier to test and debug
- ‚úÖ **Reduced memory pressure** - Process one chunk at a time instead of holding all in memory
- ‚úÖ **Leverages existing patterns** - Uses established config.py and RetryConfig patterns
- ‚úÖ **Backward compatible** - No breaking changes to API or database storage
- ‚úÖ **Aligns with deployment** - Works within existing max_concurrent_jobs=1 container design

**Cons:**
- ‚ùå **Slightly slower overall processing** - No parallelization of chunk creation (though still under 30-second target)
- ‚ùå **More complex method structure** - Additional method calls (though better organized)
- ‚ùå **Requires thorough testing** - Need to verify sequential processing works correctly

**Implementation Complexity:** Medium - Systematic refactoring with clear patterns
**Time Estimate:** 3-4 hours
**Risk Level:** Low - Building on proven patterns, backward compatible

#### Option 2: Minimal Configuration-Only Improvements  
**Approach:** Add configuration constants and retry logic but keep existing parallel processing and large methods

**Pros:**
- ‚úÖ **Faster to implement** - Minimal code changes required
- ‚úÖ **No processing logic changes** - Keeps existing parallel chunk creation
- ‚úÖ **Lower risk** - Minimal changes to working system

**Cons:**
- ‚ùå **Doesn't solve file locking** - Parallel ffmpeg processes can still conflict
- ‚ùå **Maintains complexity** - Large methods remain hard to test and debug  
- ‚ùå **Misses memory optimization** - Still holds all chunks in memory simultaneously
- ‚ùå **Limited reliability improvement** - Only adds retry logic, not structural improvements

**Implementation Complexity:** Low - Just add config constants and retry decorators
**Time Estimate:** 1-2 hours  
**Risk Level:** Low - Minimal changes

#### Option 3: Full Rewrite with Advanced Patterns
**Approach:** Complete service rewrite using worker pools, advanced async patterns, and microservice architecture

**Pros:**
- ‚úÖ **Maximum performance** - Could optimize for parallel processing without conflicts
- ‚úÖ **Advanced architecture** - Modern async patterns and worker management
- ‚úÖ **Highly scalable** - Could handle larger videos more efficiently

**Cons:**
- ‚ùå **High complexity** - Requires significant architectural changes
- ‚ùå **Breaking changes risk** - Could affect existing integrations
- ‚ùå **Over-engineering** - Adds complexity not needed for current use cases
- ‚ùå **Long implementation time** - Would require extensive testing and validation
- ‚ùå **Resource overhead** - Worker pools require more memory and management

**Implementation Complexity:** High - Complete architectural overhaul
**Time Estimate:** 8-12 hours
**Risk Level:** High - Significant changes to working system

### Recommendation & Rationale

**üéØ RECOMMENDED SOLUTION:** Option 1 - Comprehensive Refactoring with Sequential Processing

**Why this is the best choice:**
1. **Eliminates known issues** - Solves file locking, memory pressure, and maintainability problems
2. **Leverages existing patterns** - Uses established config.py and RetryConfig infrastructure  
3. **Aligns with deployment architecture** - Works perfectly with max_concurrent_jobs=1 design
4. **Balanced improvement** - Significant reliability gains without over-engineering

**Key Decision Factors:**
- **Performance Impact:** Minimal - still meets <30-second processing targets
- **Scalability:** Good - sequential processing scales better with memory constraints
- **Maintainability:** High - smaller methods easier to test, debug, and extend
- **Cost Impact:** Neutral - same resource usage, better reliability
- **Security:** Maintains existing security patterns

**Alternative Consideration:**
Option 2 is viable if time is extremely limited, but it only addresses surface issues without solving the underlying architectural problems. Option 3 would be overkill for the current requirements and deployment constraints.

### Decision Request

**üë§ USER DECISION REQUIRED:**
Based on this analysis, do you want to proceed with the recommended solution (Comprehensive Refactoring with Sequential Processing), or would you prefer a different approach?

**Questions for you to consider:**
- Are you comfortable with slightly slower chunk creation in exchange for eliminated file conflicts?
- Do you prefer the maintainability benefits of smaller, focused methods?
- Is backward compatibility important for your current deployment?

**Next Steps:**
Once you approve the strategic direction, I'll create the detailed implementation plan focusing on extending the existing VideoProcessingService with better organization and reliability.

---

## 4. Context & Problem Definition

### Problem Statement
The current VideoProcessingService works well functionally but has several maintainability and reliability issues that make it harder to debug, test, and extend. The main problems include: (1) Large monolithic methods like `_process_chunks` (100+ lines) that handle multiple responsibilities, (2) Parallel chunk creation that can cause ffmpeg file locking conflicts, (3) Hard-coded configuration values scattered throughout the code, (4) No structured retry logic for API calls, and (5) Peak memory usage from processing all chunks simultaneously. These issues reduce system reliability and make future enhancements more difficult.

### Success Criteria
- [ ] **Sequential chunk processing** eliminates file locking conflicts with ffmpeg
- [ ] **Modular method structure** with functions <50 lines each for better maintainability  
- [ ] **Configuration management** using existing config.py patterns for all constants
- [ ] **Structured retry logic** using existing RetryConfig for AI API calls
- [ ] **Memory optimization** through one-chunk-at-a-time processing
- [ ] **Backward compatibility** with existing API and database storage
- [ ] **Performance maintained** under 30-second processing time for typical videos
- [ ] **Error handling enhanced** with proper logging and graceful degradation

---

## 5. Technical Requirements

### Functional Requirements
- VideoProcessingService continues to process videos into 30-second chunks with transcription and context
- Sequential chunk processing eliminates parallel ffmpeg file access conflicts
- Configuration constants moved to config.py following existing patterns
- Retry logic applied to AI API calls using existing RetryConfig infrastructure
- Resource validation checks video duration against deployment memory limits
- All existing functionality preserved (dual embeddings, metadata, cleanup)

### Non-Functional Requirements
- **Performance:** Processing time remains under 30 seconds per video chunk, total video processing under deployment timeout limits
- **Memory:** Peak memory usage reduced through sequential processing, fits within 12Gi deployment limit
- **Reliability:** Structured retry logic for AI API calls, proper error handling with structured logging
- **Maintainability:** Methods under 50 lines each, clear separation of concerns, comprehensive docstrings
- **Scalability:** Works within existing max_concurrent_jobs=1 deployment architecture

### Technical Constraints  
- Must use existing Google GenAI and VertexAI packages and authentication patterns
- Must maintain existing ChunkData model and database storage format
- Must work within current Cloud Run deployment constraints (12Gi memory, 4 CPU, 1-hour timeout)
- Cannot modify database schema or API contracts
- Must preserve existing error handling and logging patterns

---

## 6. Data & Database Changes

### Database Schema Changes
No database schema changes required - existing `document_chunks` table structure is maintained.

### Data Model Updates
No data model changes - existing `ChunkData`, `VideoChunkMetadata`, and `TranscriptMetadata` models are preserved.

### Data Migration Plan
No data migration needed - this is a service-level refactoring that maintains all existing data formats and storage patterns.

---

## 7. API & Backend Changes

### Data Access Pattern - CRITICAL ARCHITECTURE RULES

**This is a service-layer enhancement - no API changes required.**

**BUSINESS LOGIC** ‚Üí `services/video_processing_service.py` (existing file)
- [ ] **Extend VideoProcessingService** - Refactor existing methods into smaller, focused functions
- [ ] **Sequential processing logic** - Replace parallel chunk creation with sequential workflow
- [ ] **Configuration integration** - Use config.py constants instead of hard-coded values
- [ ] **Retry logic integration** - Apply existing RetryConfig to AI API calls

**CONFIGURATION** ‚Üí `config.py` (existing file) 
- [ ] **Video processing constants** - Add VIDEO_CHUNK_DURATION_SECONDS, VIDEO_MAX_DURATION_SECONDS, etc.
- [ ] **Resource validation limits** - Based on deployment configuration analysis

### API Endpoints
No API endpoint changes - this is an internal service improvement.

### Database Operations  
No database operation changes - existing chunk storage patterns are maintained.

### External Integrations
No changes to external integrations - continues using Google GenAI for context generation and VertexAI for multimodal embeddings.

---

## 8. Python Module & Code Organization

### New Modules/Files
No new modules required - this enhances the existing `video_processing_service.py`.

### Files to Modify
- [ ] **`apps/rag-processor/rag_processor/config.py`** - Add video processing configuration constants
- [ ] **`apps/rag-processor/rag_processor/services/video_processing_service.py`** - Main refactoring target

**üö® CRITICAL: Import Pattern Requirements**
- All imports already follow correct patterns in existing codebase
- Continue using relative imports within rag_processor package
- Maintain existing external package import patterns

**Code Quality Requirements Met:**
- **Type Hints:** Existing service has complete type annotations - maintain this standard
- **Documentation:** Current docstrings are comprehensive - enhance where needed
- **Error Handling:** Good existing patterns with custom exceptions - integrate retry logic
- **Async/Await:** Already uses async patterns properly - maintain and enhance
- **Relative Imports:** Current code follows proper import patterns

### Dependency Management
**üîß NO NEW DEPENDENCIES REQUIRED:**
- [ ] **Existing packages sufficient** - Google GenAI, VertexAI, asyncio, structlog all already available
- [ ] **Retry utilities exist** - RetryConfig in utils/config_utils.py already available
- [ ] **Configuration patterns exist** - config.py already has proper structure

No pyproject.toml changes needed - all required packages and patterns already exist.

---

## 9. Implementation Plan

### Phase 1: Configuration and Constants Management
**Goal:** Move hard-coded values to config.py and add resource validation

- [ ] **Task 1.1:** Add video processing constants to config.py
  - Files: `apps/rag-processor/rag_processor/config.py`
  - Details: Add VIDEO_CHUNK_DURATION_SECONDS=30, VIDEO_MAX_DURATION_SECONDS=900, VIDEO_MAX_CHUNKS=30, VIDEO_CONTEXT_MAX_BYTES=1023, VIDEO_DEFAULT_LANGUAGE="en-US"
- [ ] **Task 1.2:** Add resource validation method to VideoProcessingService
  - Files: `apps/rag-processor/rag_processor/services/video_processing_service.py`
  - Details: Create _validate_video_resources() method to check duration and chunk count limits

### Phase 2: Method Decomposition and Organization  
**Goal:** Break large _process_chunks method into focused, maintainable functions

- [ ] **Task 2.1:** Extract chunk processing into focused methods
  - Files: `apps/rag-processor/rag_processor/services/video_processing_service.py`
  - Details: Create _process_single_chunk(), _generate_chunk_embeddings(), _create_chunk_metadata() methods
- [ ] **Task 2.2:** Implement sequential chunk processing workflow
  - Files: `apps/rag-processor/rag_processor/services/video_processing_service.py`  
  - Details: Modify _process_chunks to process one chunk completely before starting next
- [ ] **Task 2.3:** Add comprehensive error handling and logging
  - Files: `apps/rag-processor/rag_processor/services/video_processing_service.py`
  - Details: Enhanced logging for each processing step, proper cleanup in error scenarios

### Phase 3: Retry Logic and Reliability Enhancement
**Goal:** Add structured retry logic using existing RetryConfig patterns

- [ ] **Task 3.1:** Integrate RetryConfig for AI API calls
  - Files: `apps/rag-processor/rag_processor/services/video_processing_service.py`
  - Details: Apply retry logic to transcription and context generation calls
- [ ] **Task 3.2:** Enhance error handling with proper exception chaining
  - Files: `apps/rag-processor/rag_processor/services/video_processing_service.py` 
  - Details: Proper exception chaining, structured error messages, graceful degradation
- [ ] **Task 3.3:** Testing and validation of improved service
  - Files: Monitor and test with various video types and sizes
  - Details: Verify sequential processing, retry logic, and resource validation work correctly

### Code Quality Checks
**üîç MANDATORY: Run these checks after each phase:**

```bash
# Sync lint dependencies
uv sync --group lint

# Run comprehensive code quality checks
uv run --group lint ruff check apps/rag-processor/rag_processor/ --fix
uv run --group lint mypy apps/rag-processor/rag_processor/ 
uv run --group lint black --check apps/rag-processor/rag_processor/
```

---

## 10. File Structure & Organization

### New Files to Create
None - this is a refactoring of existing service.

### Files to Modify
- [ ] **`apps/rag-processor/rag_processor/config.py`** - Add video processing configuration constants
- [ ] **`apps/rag-processor/rag_processor/services/video_processing_service.py`** - Main refactoring target for method decomposition and sequential processing

### Dependencies to Add to pyproject.toml
**‚ö†Ô∏è NO NEW DEPENDENCIES REQUIRED** - All necessary packages already exist in the project.

---

## 11. Error Handling & Edge Cases

### Error Scenarios
- [ ] **Video duration exceeds limits** 
  - **Handling:** Early validation with clear error message before processing starts
- [ ] **ffmpeg chunk creation fails**
  - **Handling:** Retry with exponential backoff, then fail chunk gracefully
- [ ] **AI API calls timeout or fail**
  - **Handling:** Use RetryConfig for structured retry logic, proper error logging  
- [ ] **Memory exhaustion during processing**
  - **Handling:** Sequential processing prevents this, but add monitoring

### Edge Cases
- [ ] **Very short videos (< 30 seconds)**
  - **Solution:** Handle single chunk case gracefully
- [ ] **Videos exactly at duration limit**
  - **Solution:** Clear validation logic with proper boundary handling
- [ ] **Concurrent video processing requests**
  - **Solution:** Already handled by max_concurrent_jobs=1 deployment setting

### Custom Exception Handling
```python
# Leverage existing VideoProcessingServiceError from current code
# Add specific error types for configuration and resource validation
class VideoResourceLimitExceededError(VideoProcessingServiceError):
    """Video exceeds configured resource limits"""
    pass

class VideoConfigurationError(VideoProcessingServiceError):  
    """Video processing configuration error"""
    pass
```

---

## 12. Security Considerations

### Authentication & Authorization
- [ ] Maintains existing Google GenAI API key and VertexAI service account patterns
- [ ] No changes to authentication or authorization logic

### Input Validation  
- [ ] Enhanced resource validation for video duration and size limits
- [ ] Maintains existing file path validation and sanitization
- [ ] Uses existing Pydantic model validation patterns

### Data Protection
- [ ] Maintains existing file cleanup and temporary storage patterns
- [ ] Continues proper GenAI file upload/deletion lifecycle management
- [ ] No changes to data encryption or storage security

---

## 13. Testing Strategy (OPTIONAL)

**üìù NOTE: Testing not explicitly required for this refactoring task unless specifically needed.**

---

## 14. Deployment & Configuration

### Environment Variables
No new environment variables required - uses existing configuration structure.

### Docker Configuration
No Docker changes required - this is a service-layer improvement that works within existing container constraints.

### Health Checks (ONLY IF NEEDED)
No health check changes needed - existing service monitoring patterns sufficient.

---

## 15. Second-Order Consequences & Impact Analysis

### Impact Assessment Framework

#### 1. **Breaking Changes Analysis**
- [ ] **Existing API Contracts:** ‚úÖ No changes - VideoProcessingService API remains identical
- [ ] **Database Dependencies:** ‚úÖ No changes - ChunkData model and storage patterns unchanged
- [ ] **Service Dependencies:** ‚úÖ No changes - ProcessingService integration remains the same  
- [ ] **Authentication/Authorization:** ‚úÖ No changes - existing patterns maintained

#### 2. **Ripple Effects Assessment**  
- [ ] **Data Flow Impact:** ‚úÖ Neutral - same data structures and processing results
- [ ] **Service Integration:** ‚úÖ Transparent - ProcessingService sees no changes
- [ ] **Processing Pipeline:** ‚úÖ Improved - sequential processing more reliable and predictable
- [ ] **Error Handling:** ‚úÖ Enhanced - better retry logic and error reporting

#### 3. **Performance Implications**
- [ ] **Database Query Impact:** ‚úÖ No impact - same storage patterns
- [ ] **Memory Usage:** ‚úÖ **Reduced** - sequential processing uses less peak memory
- [ ] **API Response Times:** ‚ö†Ô∏è **Slightly slower** - sequential vs parallel chunk creation (~10-15% increase)
- [ ] **Concurrent Processing:** ‚úÖ No impact - already limited by max_concurrent_jobs=1

#### 4. **Security Considerations**
- [ ] **Attack Surface:** ‚úÖ No change - same AI API interactions
- [ ] **Data Exposure:** ‚úÖ No change - same data handling patterns
- [ ] **Input Validation:** ‚úÖ Enhanced - better resource limit validation
- [ ] **Authentication Bypass:** ‚úÖ No risk - no authentication changes

#### 5. **Operational Impact**  
- [ ] **Deployment Complexity:** ‚úÖ No change - backward compatible service improvement
- [ ] **Monitoring Requirements:** ‚úÖ Enhanced - better structured logging and error reporting
- [ ] **Resource Usage:** ‚úÖ **Improved** - reduced peak memory usage, same CPU/storage
- [ ] **Backup/Recovery:** ‚úÖ No change - same data storage patterns

#### 6. **Maintenance Burden**
- [ ] **Code Complexity:** ‚úÖ **Reduced** - smaller, focused methods easier to maintain
- [ ] **Dependencies:** ‚úÖ No change - uses existing packages
- [ ] **Testing Overhead:** ‚úÖ **Reduced** - smaller methods easier to test
- [ ] **Documentation:** ‚úÖ Enhanced - better method-level documentation

### Critical Issues Identification

#### üö® **RED FLAGS - Alert User Immediately**
**‚úÖ NONE IDENTIFIED** - This is a backward-compatible service improvement.

#### ‚ö†Ô∏è **YELLOW FLAGS - Discuss with User**  
- [ ] **Processing Time Increase:** Sequential chunk creation may increase total processing time by 10-15%
- [ ] **Code Complexity:** More method calls (though better organized)

### Mitigation Strategies

#### Performance Impact Mitigation
- [ ] **Monitoring:** Add timing metrics to verify performance stays within acceptable bounds
- [ ] **Optimization:** Sequential processing reduces memory pressure, which can improve overall performance
- [ ] **Validation:** Resource validation prevents attempting to process videos that would exceed limits

### AI Agent Checklist

- [x] **Complete Impact Analysis:** All sections analyzed - minimal negative impact identified
- [x] **Identify Critical Issues:** No red flags, minor yellow flags around performance
- [x] **Propose Mitigation:** Performance monitoring and resource validation address concerns
- [x] **Alert User:** Processing time increase is acceptable trade-off for reliability
- [x] **Recommend Alternatives:** Current approach is optimal balance of reliability vs performance

### Analysis Summary

```
üîç **SECOND-ORDER IMPACT ANALYSIS:**

**Breaking Changes Identified:**
‚úÖ None - Fully backward compatible service improvement

**Performance Implications:**  
‚ö†Ô∏è Sequential processing may increase chunk creation time by 10-15%
‚úÖ Reduced peak memory usage improves overall system stability
‚úÖ Processing still well within 30-second target and 1-hour deployment timeout

**Security Considerations:**
‚úÖ Enhanced input validation for resource limits
‚úÖ No changes to authentication or data handling patterns

**Operational Impact:**
‚úÖ Better error handling and structured logging improve observability
‚úÖ Smaller methods improve maintainability and debugging
‚úÖ No deployment complexity changes

**üö® USER ATTENTION REQUIRED:**
Sequential processing trades ~10-15% processing time increase for elimination of file locking conflicts and reduced memory pressure. This is a favorable trade-off for system reliability, and processing still meets all performance targets.
```

---

## 16. AI Agent Instructions

### Implementation Approach - CRITICAL WORKFLOW
üö® **MANDATORY: Always follow this exact sequence:**

1. **PHASE 1 FIRST (Required)**
   - [ ] Add video processing constants to config.py following existing patterns
   - [ ] Add resource validation method to check video limits before processing
   - [ ] Test configuration integration works correctly

2. **PHASE 2 SECOND (Required)**
   - [ ] Extract _process_single_chunk method to handle one chunk completely 
   - [ ] Extract _generate_chunk_embeddings method for embedding generation
   - [ ] Extract _create_chunk_metadata method for metadata creation
   - [ ] Modify _process_chunks to call new methods sequentially
   - [ ] Verify sequential processing works correctly

3. **PHASE 3 THIRD (Required)**
   - [ ] Integrate RetryConfig for AI API calls (transcription and context generation)
   - [ ] Enhance error handling with proper exception chaining
   - [ ] Add comprehensive logging for each processing step
   - [ ] Test retry logic and error handling work correctly

### Code Quality Standards
- [ ] Follow existing VideoProcessingService patterns and architectural style
- [ ] Maintain comprehensive type annotations and docstrings
- [ ] Use existing error handling patterns with VideoProcessingServiceError
- [ ] Continue existing structured logging with logger.info/error/warning patterns
- [ ] Preserve existing async/await patterns and file cleanup logic
- [ ] Integrate with existing RetryConfig and configuration patterns

### Architecture Compliance
- [ ] **‚úÖ VERIFY: Maintains existing service architecture and API contracts**
- [ ] **‚úÖ VERIFY: Uses existing config.py patterns for constants management**
- [ ] **‚úÖ VERIFY: Integrates with existing RetryConfig and error handling patterns**
- [ ] **‚ùå AVOID: Breaking changes to method signatures or return types**
- [ ] **‚ùå AVOID: Adding new dependencies or external service requirements**

### Technical Approach Validation
**CONFIRMED APPROACH:**
- ‚úÖ **Extend VideoProcessingService** using existing patterns and infrastructure
- ‚úÖ **Use config.py** for constants following established project patterns  
- ‚úÖ **Apply RetryConfig** from existing utils/config_utils.py
- ‚úÖ **Sequential processing** aligns with max_concurrent_jobs=1 deployment design
- ‚úÖ **Backward compatibility** maintained for all existing integrations

**No additional user confirmation needed** - technical approach aligns with analyzed project patterns.

---

## 17. Notes & Additional Context

### Research Links
- Video processing service analysis completed - existing code is well-structured with room for organization improvements
- Deployment configuration analysis shows 12Gi memory, 4 CPU, 1-hour timeout constraints
- RetryConfig and configuration patterns already established in project

### Performance Considerations  
- Sequential processing trades ~10-15% time increase for elimination of file conflicts
- Reduced peak memory usage from processing one chunk at a time
- Better fit for deployment constraints (max_concurrent_jobs=1)
- Processing still well within 30-second chunk targets and 1-hour total limits

### Integration Benefits
- Leverages existing config.py patterns for consistency
- Uses established RetryConfig for reliability
- Maintains all existing functionality and API contracts
- Better alignment with deployment architecture and resource constraints

---

**CRITICAL GUIDELINES:**
- **EXTEND EXISTING SERVICE** rather than rewriting - builds on proven functionality
- **USE ESTABLISHED PATTERNS** from config.py and RetryConfig infrastructure  
- **MAINTAIN BACKWARD COMPATIBILITY** - no breaking changes to existing integrations
- **SEQUENTIAL PROCESSING** eliminates file conflicts while staying within performance targets
- **IMPROVE MAINTAINABILITY** through smaller, focused methods and better organization
- **ENHANCE RELIABILITY** through structured retry logic and resource validation

---

*Template Version: 1.2*  
*Task Number: 078*  
*Last Updated: January 18, 2025*  
*Created By: AI Assistant*  
*Based on: Python Task Template* 
